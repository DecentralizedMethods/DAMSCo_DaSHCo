# Compressed Decentralized Momentum Stochastic Gradient Methods for Nonconvex Optimization


In this paper, we design two compressed decentralized algorithms for solving nonconvex
stochastic optimization under two different scenarios - DAMSCo and DaSHCo. Both algorithms adopt a momentum
technique to achieve fast convergence and a message-compression technique to save communication costs.

To run all experiments in the paper, call ./run.sh